#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åå¯„ã›ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ ã®æ¤œè¨¼
ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã‚’æœ€é©åŒ–
"""

import os
import sys
from collections import defaultdict, Counter
import pandas as pd

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from product_master.sheets_sync import GoogleSheetsSync
from core.database import supabase

def analyze_product_master():
    """å•†å“ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ"""
    print("=== å•†å“ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†æ ===\n")
    
    try:
        # Google Sheetsã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        sync = GoogleSheetsSync()
        data = sync.read_sheet('å•†å“ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°åŸºæœ¬è¡¨')
        
        if not data or len(data) < 2:
            print("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        headers = data[0]
        rows = data[1:]
        
        print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(rows)}è¡Œ")
        print(f"åˆ—æ•°: {len(headers)}åˆ—")
        print(f"ãƒ˜ãƒƒãƒ€ãƒ¼: {headers}\n")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
        df = pd.DataFrame(rows, columns=headers)
        
        # åŸºæœ¬çµ±è¨ˆ
        print("=== åŸºæœ¬çµ±è¨ˆ ===")
        for col in headers:
            if col in df.columns:
                non_empty = df[col].dropna().str.strip().str.len() > 0
                non_empty_count = non_empty.sum() if hasattr(non_empty, 'sum') else 0
                print(f"{col}: {non_empty_count}/{len(df)} ({non_empty_count/len(df)*100:.1f}%)")
        
        # å…±é€šã‚³ãƒ¼ãƒ‰åˆ†æ
        analyze_common_codes(df)
        
        # å•†å“ã‚¿ã‚¤ãƒ—åˆ†æ
        analyze_product_types(df)
        
        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥å•†å“æ•°
        analyze_platform_coverage(df)
        
        # é‡è¤‡ãƒ»æ¬ æãƒã‚§ãƒƒã‚¯
        check_data_quality(df)
        
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")

def analyze_common_codes(df):
    """å…±é€šã‚³ãƒ¼ãƒ‰ä½“ç³»ã®åˆ†æ"""
    print("\n=== å…±é€šã‚³ãƒ¼ãƒ‰åˆ†æ ===")
    
    if 'å…±é€šã‚³ãƒ¼ãƒ‰' not in df.columns:
        print("å…±é€šã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    codes = df['å…±é€šã‚³ãƒ¼ãƒ‰'].dropna().str.strip()
    codes = codes[codes.str.len() > 0]
    
    print(f"å…±é€šã‚³ãƒ¼ãƒ‰ç·æ•°: {len(codes)}")
    print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {codes.nunique()}")
    print(f"é‡è¤‡æ•°: {len(codes) - codes.nunique()}")
    
    # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹åˆ†æ
    prefixes = codes.str[:2].value_counts()
    print(f"\nãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹åˆ†å¸ƒ:")
    for prefix, count in prefixes.items():
        percentage = count / len(codes) * 100
        print(f"  {prefix}: {count}ä»¶ ({percentage:.1f}%)")
    
    # é•·ã•åˆ†æ
    lengths = codes.str.len().value_counts().sort_index()
    print(f"\nã‚³ãƒ¼ãƒ‰é•·åˆ†å¸ƒ:")
    for length, count in lengths.items():
        print(f"  {length}æ¡: {count}ä»¶")

def analyze_product_types(df):
    """å•†å“ã‚¿ã‚¤ãƒ—ã®åˆ†æ"""
    print("\n=== å•†å“ã‚¿ã‚¤ãƒ—åˆ†æ ===")
    
    if 'å•†å“ã‚¿ã‚¤ãƒ—' in df.columns:
        types = df['å•†å“ã‚¿ã‚¤ãƒ—'].dropna().str.strip()
        type_counts = types.value_counts()
        print("æ˜ç¤ºçš„ãªå•†å“ã‚¿ã‚¤ãƒ—:")
        for type_name, count in type_counts.items():
            print(f"  {type_name}: {count}ä»¶")
    
    # å…±é€šã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹å•†å“ã‚¿ã‚¤ãƒ—
    if 'å…±é€šã‚³ãƒ¼ãƒ‰' in df.columns:
        codes = df['å…±é€šã‚³ãƒ¼ãƒ‰'].dropna().str.strip()
        inferred_types = codes.apply(infer_product_type)
        inferred_counts = inferred_types.value_counts()
        
        print(f"\nå…±é€šã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹å•†å“ã‚¿ã‚¤ãƒ—:")
        for type_name, count in inferred_counts.items():
            print(f"  {type_name}: {count}ä»¶")

def infer_product_type(code):
    """å…±é€šã‚³ãƒ¼ãƒ‰ã‹ã‚‰å•†å“ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
    if pd.isna(code) or len(str(code).strip()) == 0:
        return "ä¸æ˜"
    
    code = str(code).strip().upper()
    if code.startswith('CM'):
        return 'å˜å“'
    elif code.startswith('BC'):
        return 'ã‚»ãƒƒãƒˆå•†å“'
    elif code.startswith('PC'):
        return 'ã¾ã¨ã‚å•†å“'
    else:
        return 'ãã®ä»–'

def analyze_platform_coverage(df):
    """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥å•†å“ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
    print("\n=== ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ ===")
    
    platform_columns = [
        ('æ¥½å¤©SKU', 'æ¥½å¤©'),
        ('ã‚«ãƒ©ãƒ¼ãƒŸãƒ¼ID', 'ã‚«ãƒ©ãƒ¼ãƒŸãƒ¼'),
        ('ã‚¹ãƒãƒ¬ã‚¸ID', 'ã‚¹ãƒãƒ¬ã‚¸'),
        ('Yahooå•†å“ID', 'Yahoo'),
        ('Amazon ASIN', 'Amazon'),
        ('ãƒ¡ãƒ«ã‚«ãƒªå•†å“ID', 'ãƒ¡ãƒ«ã‚«ãƒª')
    ]
    
    total_products = len(df)
    
    for col_name, platform_name in platform_columns:
        if col_name in df.columns:
            non_empty = df[col_name].dropna().str.strip()
            non_empty = non_empty[non_empty.str.len() > 0]
            coverage = len(non_empty) / total_products * 100
            print(f"{platform_name}: {len(non_empty)}/{total_products} ({coverage:.1f}%)")

def check_data_quality(df):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ ===")
    
    issues = []
    
    # å…±é€šã‚³ãƒ¼ãƒ‰ã®é‡è¤‡
    if 'å…±é€šã‚³ãƒ¼ãƒ‰' in df.columns:
        codes = df['å…±é€šã‚³ãƒ¼ãƒ‰'].dropna().str.strip()
        codes = codes[codes.str.len() > 0]
        duplicates = codes[codes.duplicated()].unique()
        if len(duplicates) > 0:
            issues.append(f"é‡è¤‡ã™ã‚‹å…±é€šã‚³ãƒ¼ãƒ‰: {len(duplicates)}ä»¶")
            print(f"é‡è¤‡å…±é€šã‚³ãƒ¼ãƒ‰: {list(duplicates)[:5]}...")
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¬ æ
    required_fields = ['å…±é€šã‚³ãƒ¼ãƒ‰', 'åŸºæœ¬å•†å“å']
    for field in required_fields:
        if field in df.columns:
            empty_count = df[field].isna().sum() + (df[field].str.strip() == '').sum()
            if empty_count > 0:
                issues.append(f"{field}ãŒç©º: {empty_count}ä»¶")
    
    # å•†å“åã®é‡è¤‡
    if 'åŸºæœ¬å•†å“å' in df.columns:
        names = df['åŸºæœ¬å•†å“å'].dropna().str.strip()
        duplicate_names = names[names.duplicated()].unique()
        if len(duplicate_names) > 0:
            issues.append(f"é‡è¤‡ã™ã‚‹å•†å“å: {len(duplicate_names)}ä»¶")
    
    if issues:
        print("ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:")
        for issue in issues:
            print(f"  âŒ {issue}")
    else:
        print("âœ… ä¸»è¦ãªå“è³ªå•é¡Œã¯ç™ºè¦‹ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

def analyze_choice_codes():
    """é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨ã®åˆ†æ"""
    print("\n=== é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰åˆ†æ ===")
    
    try:
        sync = GoogleSheetsSync()
        data = sync.read_sheet('é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨')
        
        if not data or len(data) < 2:
            print("é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        headers = data[0]
        rows = data[1:]
        df = pd.DataFrame(rows, columns=headers)
        
        print(f"é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰ç·æ•°: {len(df)}ä»¶")
        
        if 'é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰' in df.columns and 'æ–°å…±é€šã‚³ãƒ¼ãƒ‰' in df.columns:
            choice_codes = df['é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰'].dropna().str.strip()
            common_codes = df['æ–°å…±é€šã‚³ãƒ¼ãƒ‰'].dropna().str.strip()
            
            print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰: {choice_codes.nunique()}ä»¶")
            print(f"å¯¾å¿œã™ã‚‹å…±é€šã‚³ãƒ¼ãƒ‰: {common_codes.nunique()}ä»¶")
            
            # 1ã¤ã®å…±é€šã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰æ•°
            code_mapping = df.groupby('æ–°å…±é€šã‚³ãƒ¼ãƒ‰')['é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰'].count()
            print(f"\nå…±é€šã‚³ãƒ¼ãƒ‰ã‚ãŸã‚Šã®é¸æŠè‚¢æ•°:")
            print(f"  å¹³å‡: {code_mapping.mean():.1f}å€‹")
            print(f"  æœ€å¤§: {code_mapping.max()}å€‹")
            print(f"  æœ€å°: {code_mapping.min()}å€‹")
        
    except Exception as e:
        print(f"é¸æŠè‚¢ã‚³ãƒ¼ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")

def analyze_package_components():
    """ã¾ã¨ã‚å•†å“å†…è¨³ã®åˆ†æ"""
    print("\n=== ã¾ã¨ã‚å•†å“å†…è¨³åˆ†æ ===")
    
    try:
        sync = GoogleSheetsSync()
        data = sync.read_sheet('ã¾ã¨ã‚å•†å“å†…è¨³ãƒ†ãƒ¼ãƒ–ãƒ«')
        
        if not data or len(data) < 2:
            print("ã¾ã¨ã‚å•†å“ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¢ã™
        header_row = None
        for i, row in enumerate(data):
            if any('å†…è¨³ID' in str(cell) for cell in row):
                header_row = i
                break
        
        if header_row is None:
            print("ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        headers = data[header_row]
        rows = data[header_row + 1:]
        df = pd.DataFrame(rows, columns=headers)
        
        print(f"ã¾ã¨ã‚å•†å“å†…è¨³ç·æ•°: {len(df)}ä»¶")
        
        if 'ã¾ã¨ã‚å•†å“å…±é€šã‚³ãƒ¼ãƒ‰' in df.columns:
            package_codes = df['ã¾ã¨ã‚å•†å“å…±é€šã‚³ãƒ¼ãƒ‰'].dropna().str.strip()
            package_codes = package_codes[package_codes.str.len() > 0]
            
            print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ã¾ã¨ã‚å•†å“: {package_codes.nunique()}ä»¶")
            
            # ã¾ã¨ã‚å•†å“ã‚ãŸã‚Šã®æ§‹æˆå“æ•°
            component_counts = df.groupby('ã¾ã¨ã‚å•†å“å…±é€šã‚³ãƒ¼ãƒ‰').size()
            print(f"\nã¾ã¨ã‚å•†å“ã‚ãŸã‚Šã®æ§‹æˆå“æ•°:")
            print(f"  å¹³å‡: {component_counts.mean():.1f}å€‹")
            print(f"  æœ€å¤§: {component_counts.max()}å€‹")
            print(f"  æœ€å°: {component_counts.min()}å€‹")
        
    except Exception as e:
        print(f"ã¾ã¨ã‚å•†å“åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")

def check_database_consistency():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ ===")
    
    try:
        # ç¾åœ¨ã®DBå†…å®¹ã‚’ç¢ºèª
        db_products = supabase.table('product_master').select('common_code', 'product_name', 'product_type').execute()
        
        if db_products.data:
            print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…å•†å“æ•°: {len(db_products.data)}ä»¶")
            
            db_codes = set(p['common_code'] for p in db_products.data)
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¨ã®æ¯”è¼ƒ
            sync = GoogleSheetsSync()
            sheet_data = sync.read_sheet('å•†å“ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°åŸºæœ¬è¡¨')
            
            if sheet_data and len(sheet_data) > 1:
                headers = sheet_data[0]
                rows = sheet_data[1:]
                
                if 'å…±é€šã‚³ãƒ¼ãƒ‰' in headers:
                    code_idx = headers.index('å…±é€šã‚³ãƒ¼ãƒ‰')
                    sheet_codes = set()
                    for row in rows:
                        if len(row) > code_idx and row[code_idx]:
                            sheet_codes.add(str(row[code_idx]).strip())
                    
                    print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå†…å•†å“æ•°: {len(sheet_codes)}ä»¶")
                    
                    # å·®åˆ†åˆ†æ
                    only_in_db = db_codes - sheet_codes
                    only_in_sheet = sheet_codes - db_codes
                    
                    if only_in_db:
                        print(f"DBã®ã¿ã«å­˜åœ¨: {len(only_in_db)}ä»¶")
                    if only_in_sheet:
                        print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã¿ã«å­˜åœ¨: {len(only_in_sheet)}ä»¶")
                    
                    if not only_in_db and not only_in_sheet:
                        print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¯åŒæœŸã•ã‚Œã¦ã„ã¾ã™")
        else:
            print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")

def recommend_database_improvements():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ”¹å–„ææ¡ˆ"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ”¹å–„ææ¡ˆ ===")
    
    recommendations = [
        "1. å…±é€šã‚³ãƒ¼ãƒ‰ã«åˆ¶ç´„ã‚’è¿½åŠ ï¼ˆæ­£è¦è¡¨ç¾ãƒã‚§ãƒƒã‚¯ï¼‰",
        "2. å•†å“ã‚¿ã‚¤ãƒ—ã®ENUMåˆ¶ç´„ã‚’è¿½åŠ ",
        "3. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰IDã®é‡è¤‡ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½",
        "4. ãƒ‡ãƒ¼ã‚¿å¤‰æ›´å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¿½åŠ ",
        "5. è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ã®å®Ÿè£…"
    ]
    
    for rec in recommendations:
        print(f"  ğŸ“‹ {rec}")

if __name__ == "__main__":
    print("åå¯„ã›ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹...\n")
    
    analyze_product_master()
    analyze_choice_codes()
    analyze_package_components()
    check_database_consistency()
    recommend_database_improvements()
    
    print("\nåˆ†æå®Œäº†ï¼")